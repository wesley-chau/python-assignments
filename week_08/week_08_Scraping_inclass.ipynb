{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Web Scraping\n",
    "\n",
    "\n",
    "Scraping is not always legal!  \n",
    "\n",
    "Some rules to consider: \n",
    "* Be respectful and do not bombard a website with scraping request or else you can get your IP address blocked\n",
    "* Check the website permission before you begin! If there is an API available, use it. Most websites won't let you use their data commercially.\n",
    "* Each website is unique and may update, so you may need to update your code and/or customize your scraping code for each website\n",
    "\n",
    "\n",
    "When is it a good idea to scrape a website:\n",
    "* API is not available, or information you want is not in the API\n",
    "* You want to anonoymously scrape a website (use a VPN) \n",
    "\n",
    "Here is a Web Scraping Sandbox where you can practice scraping: \n",
    "http://toscrape.com/\n",
    "\n",
    "Today, we're going to start with scraping www.wikipedia.com because it is *legal* to scrape\n",
    "\n",
    "Make sure you download requests and bs4 via terminal \n",
    "\n",
    "* pip install requests\n",
    "* pip install bs4\n",
    "\n",
    "or if you're using Anaconda \n",
    "\n",
    "* conda install requests\n",
    "* conda install bs4\n",
    "\n",
    "or install it via notebook \n",
    "\n",
    "* !pip install requests\n",
    "* !pip install bs4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The request library will grab the page\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The beautifulsoup library makes your code legible and helps you analyze the extracted page\n",
    "\n",
    "import bs4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next inspect the elements on the wiki page, I want to grab the headlines \n",
    "# the headlines are in the class=\"mw-headline\" in a <span> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for the scrapped headlines \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save to a CSV, we first want to create a dataframe for the data\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're starting by going through the HTML and looking for something all the images have in common \n",
    "# the class 'thumbimage' applies to all the images \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're creating a list of the links for the thumbnails \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request #to download \n",
    "import urllib.parse # to download \n",
    "from urllib.error import HTTPError # to see error \n",
    "import time # to keep track of time \n",
    "import random as r # to donwload at random time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to download imgs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enumerate() take 2 parameters, iterable, start (optional)\n",
    "#iterable iterates through the object (in our case, a list of links)\n",
    "\n",
    "#first, create folder called \"images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
